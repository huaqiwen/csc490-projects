\documentclass[letter]{article}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{ifthen}
\usepackage{fancyhdr}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage[shortlabels]{enumitem}

\newcommand{\fref}[1]{\textbf{Figure \ref{#1}}}

%%%
% Set up the margins to use a fairly large area of the page
%%%
\oddsidemargin=.2in
\evensidemargin=.2in
\textwidth=6in
\topmargin=0in
\textheight=9.0in
\parskip=.07in
\parindent=0in
\pagestyle{fancy}

%%%
% Set up the header
%%%
\newcommand{\setheader}[6]{
	\lhead{{\sc #1} {\sc #2}\\{} ({\small \it \today})}
	\rhead{
		{\bf #3} 
		\ifthenelse{\equal{#4}{}}{}{(#4)}\\
		{\bf #5}
		\ifthenelse{\equal{#6}{}}{}{(#6)}%
	}
}

\begin{document}
	\setheader{CSC490}{Module 1}{Ben Weisz}{}{Qiwen Hua}{}

	Note: We appologize for the length of the report but the figures take up quite a bit of space and add great value to the report.
	\section{Focal Loss \& Anisotropic Guassian}

	\subsection{Motivation}
	During our evaluation of the original algorithm designed for this module, we noticed the following three things:
	\begin{itemize}
		\item Among the top detections (detections with highest detection scores), we get a considerably lower precision score for the recall aggregates within the recall range of 0 and 0.1 than for range 0.1 to 0.2. What this tells us is that the false positive rates are higher among the top detections than in the 0.1 to 0.2 recall range. This can be seen in the 0.1 to 0.2 range on the horizontal axis in original PR Curve figure in report A for all values of $\tau$.
		\item As we sweep through the detection scores, the true and false positives are not evenly distributed in the aggregations. We see this in the fact that as we sweep through the detection scores the the curve has a noticeable negative slope. The reason that this may be bad is that we would ideally hope to see a higher rate of true positives even for lower detection scores. Since we get more false positives for the lower detection scores we can see that our current model does not match the ideal case.
		\item As we sweep through the detection scores we see that the recall scores tend to drop gradually as we decreases the scores. Ideally we would want to see these scores not decrease and then there be an abrupt decrease. This would mean that our model's recall is higher for more detections.
	\end{itemize}

	By tackling the above three problems we hope to decrease our model's false positive and negative rates. By doing this we would increase both our recall and precision, thus leading to a better model overall.\\\\
	We will now present an overview of our technique, describing it in more detail in the techniques section. We use an anisotropic gaussian to model the vehicles for the target heatmap. We then use focal loss as our new loss function. This combination of the anisotropic gaussian and the focal loss function were chosen to hopefully allow the model to better orient the vehicles and because focal loss helps the model focus on under represented classification cases.\\\\
	Alternatives to our techniques to tackling these issues will be presented at the end our our techniques section as future work.

	\subsection{Techniques}
	\textbf{Anistropic Gaussian:}\\
    The intuition behind using an anisotropic gaussian to model the the vehicle target heatmap is that we want the model to have a better understanding of the rotation and scale of the vehicles from the heatmap. An example output of of the target heatmap is show below in \fref{fig:aniso}. During inference we use the heatmap to determine the point where a vehicle is located in the frame. We want to embue the predicted heatmap with orientation as this will help us better localize the vehicles, orient them and scale them. While we did get to implement the anisotropic gaussian, we did not have enough time to correctly make use of the predicted heatmap to help with orientation and scaling. The idea is, that with the predicted heatmap of oriented gaussians we fit a small GMM with one peak with a few iterations at infernce time to the region around the localized vehicle. Once fitted, the covariance matrix can be decomposed in to matricies for orientation and scale using singular value decomposition. These values for orientation and scale could then be combined with the regressed orientation and scale for a hopefully more accurate prediction.

	\begin{figure}[h]
		\begin{subfigure}[t]{0.49\textwidth}
			\centering
			\includegraphics[scale=0.4]{images/aniso_target.png}
			\caption{Sample frame heatmap target.}
		\end{subfigure}
		\begin{subfigure}[t]{0.49\textwidth}
			\centering
			\includegraphics[scale=0.4]{images/aniso_pred.png}
			\caption{Sample frame heatmap prediction.}
		\end{subfigure}
		\vspace*{1mm}
		\caption{Heatmaps on frames from the training sequences}
		\label{fig:aniso}
	\end{figure}

	Breifly, we describe the method for generating the anisotropic gaussian targets. In order to properly capture the scale and orientation of vechiles in their location we wanted to gaurentee that at least 95\% of the point cloud points associated with a vehicle would be gauranteed to be within $2\sigma$ of the center location.
	\begin{align}
            2 \sigma_x &= \frac{s_x}{2}\\
			\sigma_x &= \frac{s_x}{4}
	\end{align}
	This gives us a value of $\sigma_x = \frac{s_x}{4}$ and $\sigma_y = \frac{s_y}{4}$ for the respective directions. These can then be placed along the diagonal of a matrix $S$ respectively with all other values being zero. The rotation matrix for the given yaw $\beta$ can be created using the standard 2 x 2 rotation matrix $R$. The covariance for the target gaussian can then be created as follows:
	\begin{align}
		\textstyle \sum = R S S R^\top
	\end{align}
	The anisotropic gaussian is then created using the standard multivariate gaussian formula, with the mean being the center locations of the vehicles.
	\begin{equation}
		\mathcal{N}(x, \textstyle \sum, \mu) = \frac{1}{(2\pi)^\frac{n}{2}}|\textstyle \sum|^{-\frac{1}{2}} exp (-\frac{1}{2}(x-\mu)^\top \textstyle \sum^{-1}(x - \mu))
	\end{equation}
	\textbf{Focal Loss:}\\
	Our intuition behind using focal loss is that we want to fix the inbalance between the types of examples the model sees and trains on. To stop model from localizing too many false positives and missing too may labels we weight these types of detections heavier in the loss function. This puts more emphasis on training the model to correctly classify these harder to detect cases.\\\\
	We use the Focal Loss suggested in \cite{objects-as-points} to compute the loss for our heatmap.
	\begin{equation}
		L_k = -\frac{1}{N} \sum_{xy} 
		\begin{cases} 
			(1 - \hat{Y}_{xy})^\alpha log (\hat{Y}_{xy}) & Y_{xy} > \tau\\
			(1 - Y_{xy})^\beta (\hat{Y}_{xy})^\alpha log(1 - \hat{Y}_{xy})& otherwise \\
		 \end{cases}
	\end{equation}
	Where $Y_{xy}, \hat{Y}_{xy}$ are the target and predicted heatmap outputs respectively, with $\tau$ representing a heatmap threshold and $\alpha, \beta$ being focal loss hyper parameters. $\tau, \alpha, \beta$ were tuned to $0.1$, $2.0$ and $4.0$ through testing and by recommendation in \cite{objects-as-points}. \\\\
	The regressed variables for scale, orientation and location are accounted for with an L1 loss function as recommended in \cite{objects-as-points}.\\\\
	These are then combined into the loss and weighted with parameters $\lambda_{k}, \lambda_{ori}, \lambda_{size}, \lambda_{loc}$ with tuned values of 100.0, 100.0, 1.0, 10.0 respectively.
	\begin{align}
		L = \lambda_k L_k + \lambda_{ori} L^1_{ori} + \lambda_{size} L^1_{size} + \lambda_{loc} L^1_{loc}
	\end{align}
	\textbf{Further Work:}\\
	One approach that could be taken to tackle some of the issues with the current model would be to try to remove some labels in the raw data that have no overlapping point cloud data. In \fref{fig:false-positive} we can see that there are a lot of labels at the top of the frame were there is no point cloud data, thus making it hard for the model to learn these. In essence, using these labels currently trains the model to generate false positives as detections like these could be generated anywhere in the frame that has no points.\\\\
	To combat this, when building the targets, remove labels which have no point cloud data and train on the remaining labels.
	\begin{figure}[h]
		\centering
		\includegraphics[scale=0.35]{images/false-positives.png}
		\caption{False positives along top of frame}
		\label{fig:false-positive}
	\end{figure}

	\subsection{Evaluation}
	We analyzed the quality of the model by generating the PR curve for the new model using the same process as for the original model because our goals were to decrease the false positives and false negatives and to increase the number of true positives.\\\\
	Below we can see that by using focal loss we've managed to smooth out the jump in precision among the highest detections (except for $\tau=2.0$). Overall, compared to the original model, the false positives are more distributed amongst the true positive detections. This can be seen in the charts below in the fact that the PR curves have a more flatline slope for recall ranges between 0.1 and 0.4.

	\begin{figure}[!h]
		\centering
		\begin{subfigure}[t]{0.4\textwidth}
			\centering
			\includegraphics[width=\linewidth]{images/plot_tau_2_focal.eps}
			\caption{PR Curve with $\tau=2.0$}
		\end{subfigure}
		\begin{subfigure}[t]{0.4\textwidth}
			\centering
			\includegraphics[width=\linewidth]{images/plot_tau_4_focal.eps}
			\caption{PR Curve with $\tau=4.0$}
		\end{subfigure}
		\vspace*{1mm}
	  
		\begin{subfigure}[t]{0.4\textwidth}
			\centering
			\includegraphics[width=\linewidth]{images/plot_tau_8_focal.eps}
			\caption{PR Curve with $\tau=8.0$}
		\end{subfigure}
		\begin{subfigure}[t]{0.4\textwidth}
			\centering
			\includegraphics[width=\linewidth]{images/plot_tau_16_focal.eps}
			\caption{PR Curve with $\tau=16.0$}
		\end{subfigure}

		\caption{PR Curves for varying values of $\tau$ with focal loss}
		\label{fig:prcurvefocal}
	\end{figure}

	We can also see that the AP scores have also slightly increased for each value of $\tau$. Meaning that both the precision and recall have increased. This can be seen in $\textbf{Table \ref{tab:auc}}$.
	\begin{table}[!ht]
		\centering
		\begin{tabular}{|c|c|c|c|c|c|}
		\hline
		\textbf{Values of $\tau$} & 2.0 & 4.0 & 8.0 & 16.0 & \textbf{Mean} \\ \hline
		\textbf{AP for original Model} & 0.3792 & 0.4521 & 0.4609 & 0.4539 & 0.4365 \\ \hline
		\textbf{AP with Focal Loss} & 0.3944 & 0.4625 & 0.4702 & 0.4599 & 0.4468 \\ \hline
		\end{tabular}
		\caption{\label{tab:auc} AUC values for varying distance thresholds $\tau$}
	\end{table}

	In \fref{fig:comp-det} we can qualitatively see that the new model is detecting more true positives than the old model. 

	\begin{figure}[!h]
		\centering
		\begin{subfigure}[t]{0.49\textwidth}
			\centering
			\includegraphics[width=\linewidth]{images/000.png}
			\caption{Detections of OLD model in test frame 000}
		\end{subfigure}
		\begin{subfigure}[t]{0.49\textwidth}
			\centering
			\includegraphics[width=\linewidth]{images/000-focal.png}
			\caption{Detections of NEW model in test frame 000}
		\end{subfigure}
		\caption{Comparision of detections for test 000.}
		\label{fig:comp-det}
	\end{figure}

	\subsection{Limitations}
	Since we did not get to completely implement the inferencing with the anisotropic gaussian, we can see that there is a failure case that arises where objects that are further away cannot be properly orientated. An example of this can be seen in \fref{fig:failure} on the right hand side in the center. The green car is orientated parallel to the self driving car and but the detection says that it is roughly at a 45 degree angle.

	\begin{figure}[h]
		\centering
		\includegraphics[scale=0.35]{images/failure.png}
		\caption{Detection orientation failure case.}
		\label{fig:failure}
	\end{figure}
	
	One potential improvement that could be made would be feed multiple frames into the model at training time. Similar to how stereo vision allows one to triangulate objects in three dimensions, using multiple frames for one classification would allow the model to better understand where objects are, their size and their orientation. This could significantly improve the model's accuracy on far away objects as it can take multiple lidar measurements of these objects which is often needed due to the lack of data due to distance from the vehicle.

	\section{Multi-class Detector}

    In this section, we will discuss details regarding extending our detector system to detect more than one class of objects (cars in Part A). 

	\subsection{Motivation}

    After finishing Part A of this module, our detector possesses the ability to detect cars with LiDAR point clouds. However, in the real world applications of self driving car perceptions, detecting only one class is insufficient, as there are many more important classes of objects surrounding the car: pedestrians, signs, bicycles, signals, etc. 
    
    This problem is relevant as the number of classes of objects our system can detect is directly related with how well our system perceives the world. In other word, tackling this problem directly improves the safety and practicality of our detector system. 

    We choose to move forward with this idea as we not only want our detector system to be accurate but also to be well-rounded. Having our system to be able to detect multiple classes makes it more functional in the real world. 

	\subsection{Techniques}

    Naturally, the traits (e.g., sizes, average movement speeds, frequently appearing locations) of each classes of objects differ wildly. For example, the BEV sizes of vehicles and pedestrians can differ by a factor of 51 ($ 1.7m \times 4.5m $ car and $ 0.5m \times 0.3m $ human). Therefore, if we want our system to be able to predict $ N_c $ object classes, our approach would be to build $ N_c $ detection models each being responsible to detect one corresponding object class. This intuition is similar to one-vs-rest classification where we use multiple binary classifications for multi-class classification. 

	Since we are building multiple single-class detection models, we do not need to modify the existing algorithm for the \verb|DetectionModelConfig| and \verb|DetectionModel| Python classes. However, we need to make changes to the following components: \begin{enumerate}
		\item \textbf{Dataset:} First, we specify the classes of interest in variable \verb|DETECTING_CLASSES| in \\
		\verb|detection/pandaset/dataset.py|. All entrypoints of the detector system use the \verb|pytorch|'s \verb|DataLoader| class to load the dataset of type \verb|PandasetDataset|. Therefore, to include data points from all interested classes, the first step would be to modify the \verb|PandasetDataset| class. 
		
		The \verb|__getitem__| method originally returns the BEV voxel grid tensor, the ground truth target tensor, and a set of ground truth BEV detections of class \verb|Dectections|. While the first item (BEV voxel grid) is class independent, the latter two only describes the vehicles. We modifed the last two return types of this function to be dictionaries with the keys being the interested object classes and the values being the original ground truth target tensors and detections. 

		In addition, we need to modify the \verb|custom_collate| function slightly to be compatible with the modified \verb|__getitem__| method. 

		\item \textbf{Visualization:} We want our detection visualizations to draw detections of all interested classes in one single figure. Therefore, we updated the \verb|visualize_detections| function take in dictionaries of both detections and labels input. In the function body, we iterate through the dictionaries and use the original algorithm to plot the objects while changing the color slightly at each iteration to distinguish between the different object classes. 
		
		\item \textbf{Overfitting and training:} With all the preparations finished, we can now modify the \verb|overfit| and \verb|train| functions in \verb|main.py|. These two functions are similar as they both need to train on different classes. First, we modified the implementations to initialize models, loss functions, and optimizers for each classes and store them in three dictionaries. Then, we loop over the interested classes on top of the previous training loops and train on the corresponding class-specific models. Once an iteration finish, i.e., a single-class model has finished training, we store the model checkpoint. In the end, we will have stored $ N_c $ model checkpoints. 
		
		After the training is complete, we build a dictionary of detections by iterating over the interested classes again and use the corresponding models to make inferences. Finally, the combined detections dictionary is sent to the \verb|visualize_detections| function mentioned earlier.

		\item \textbf{Testing:} Now that we have trained our models and stored the model checkpoint for each object classes, we can test our model with \verb|test| with some modifications. First, since we have one single-class model for each object class, we need to correspondingly supply more than one model checkpoints. Our current implementation uses the substring \verb|"\\"| as seperators to pass in multiple checkpoint paths as a single string. Note that we can also pass in a single path to a file with one checkpoint path at each line; this method would be better if we have many interested object classes. 
		
		After parsing the model checkpoints and initializing the corresponding models, we loop over the interested classes and make inferences in each frame iteration. The detections for all classes are combined as discussed previously, and the system visualizes a single plot for each frame. 

	\end{enumerate}

	\subsection{Evaluation}

	Unfortunately, the number of samples are not abundant for classes other than cars in the current dataset (will be discussed more later in \S 2.4 Limitations). Therefore, we need to train the models over a large number of epochs to obtain meaningful and reasonably representative models. However, this is currently not feasible due to hardware limitations, i.e., the OS consistently sends \verb|SIGKILL| to the training process halfway due to extreme resource starvation. 

	Thus, the only way to evaluate our multi-class detector is to see the overfit result of a single frame. Here we will focus specifically at the pedestrians class. Since the pedestrian objects are much smaller than the car objects, it would naturally take more iterations for the detector to converge on the heatmap. Therefore, we set the iterations count to 1,500 and set the \verb|score_threshold| hyperparameter to -3.9 and start overfitting. Note that we have chosen frame 200 as this frame has a large number of pedestrians. 
	
	\begin{figure}[h]
		\begin{subfigure}[t]{0.49\textwidth}
			\centering
			\includegraphics[width=\linewidth]{images/hm_overfit_target_pedes.png}
			\caption{Target pedestrians heatmap}
			\label{fig:hmpedes_a}
		\end{subfigure}
		\begin{subfigure}[t]{0.49\textwidth}
			\centering
			\includegraphics[width=\linewidth]{images/hm_overfit_pedes.png}
			\caption{Predicted pedestrians heatmap}
			\label{fig:hmpedes_b}
		\end{subfigure}

		\caption{Pedestrians heatmap after overfitting frame 200 of PandaSet}
		\vspace*{3mm}
	\end{figure}

	\begin{figure}[h]
		\centering
		\includegraphics[width=\linewidth]{images/det_overfit_pedes.png}
		\caption{Pedestrians detections after overfitting frame 200 of PandaSet}
		\label{fig:detpedes}
	\end{figure}

	In \fref{fig:hmpedes_a} and \fref{fig:hmpedes_b}, we can see that the predicted heatmap resembles some traits of the target heatmap. However, since the pedestrians are so small in the voxel grid and are often gathered closely, the predicted heatmap can only detect the locations of clusters of pedestrians. 

	The results of the pedestrian detections are shown in \fref{fig:detpedes} with the green boxes being the ground truth labels and red boxes being the detections. As mentioned before, since our detection heatmap can only detect clusters of small objects (in this case, pedestrians), the detector only makes one detection per cluster. Thus we can see that there are only one red box for each closely positioned pedestrian groups. 

	\subsection{Limitations}

	As mentioned in \S 2.3 Evaluation, due to limited number of observations, the model performs poorly if the number of iterations (or epochs) is not large enough. In addition, the current neural network setup does not work well with small objects sizes. To better detect small objects such as pedestrians and signs, we may need different hidden layers in the current neural network. With these current limitations in mind, we can improve our multi-class detector with the following potential paths: \begin{enumerate}
		\item Tune a different set of hyperparameters for each object class with a validation dataset;
		\item Train the model with a larger dataset and a larger step size if the sample size is small;
		\item Explore other hidden layers that may be more suitable for detecting smaller objects. 
	\end{enumerate}
	
	\begin{thebibliography}{9}
		\bibitem{objects-as-points}
		Xingyi Zhou and Dequan Wang and Philipp Kr{\"{a}}henb{\"{u}}hl (2019) \emph{Objects as Points}, CoRR.
	\end{thebibliography}

\end{document}

